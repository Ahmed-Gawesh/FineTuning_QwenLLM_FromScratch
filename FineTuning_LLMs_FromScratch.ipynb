{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgZVwaPy_P5C",
        "outputId": "fe9bbcc8-3dc8-4db5-88eb-74bd0fe21b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Libraries\n"
      ],
      "metadata": {
        "id": "VPvLFCeia--5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install groq  wandb\n",
        "!pip install json_repair"
      ],
      "metadata": {
        "id": "_Yd9HHwUbDZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU faker\n",
        "!pip install -qU vllm\n",
        "!pip install json_repair"
      ],
      "metadata": {
        "id": "in1Aol7j-yhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e ."
      ],
      "metadata": {
        "id": "Ea33C_OLYhW2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "wandb.login(key = userdata.get('wandb'))\n",
        "hf_token = userdata.get('huggingface')\n",
        "!huggingface-cli login --token {hf_token}"
      ],
      "metadata": {
        "id": "6hS_eL3icPEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "tt7MgjJcdnDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import requests\n",
        "import random\n",
        "from tqdm.auto import  tqdm\n",
        "\n",
        "# for Creating Schema\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "import json_repair\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "import time\n",
        "# from groq import APIStatusError"
      ],
      "metadata": {
        "id": "KgoHzQXLdqHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Links\n",
        "data_dir = '/gdrive/MyDrive/FineTuning_fromScratch'\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "def repaire_json(article_text):\n",
        "  try:\n",
        "    return json_repair.loads(article_text)\n",
        "  except:\n",
        "    return None"
      ],
      "metadata": {
        "id": "7WVImt67d7N2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"\n",
        "في أبريل/نيسان 2022، دخل شخصان مكتبة جامعة تارتو في إستونيا، وطلبا الاطلاع على ثمانية كتب لفنانين من القرن التاسع عشر: الشاعر والكاتب المسرحي الروسي ألكسندر بوشكين، والكاتب الروسي الشهير من أصل أوكراني نيكولاي غوغول.\n",
        "\n",
        "تحدث الرجلان بالروسية، مؤكدين لموظف المكتبة أن الأصغر سناً منهما يُجري بحثاً لإكمال دراسته في الولايات المتحدة.\n",
        "\n",
        "بعد ثلاثة أشهر، لاحظ موظفو المكتبة استبدال كتابين من الكتب الثمانية بنسخ مزيفة. وكشفت سجلات المكتبة أن الكتابين الأصليين كانا قد خرجا للمرة الأخيرة بحوزة نفس الرجلين في أبريل/نيسان.\n",
        "\n",
        "سارع أمناء المكتبة إلى فحص الكتب الستة الأخرى التي تعد من الكلاسيكيات الروسية. وبعد فحص دقيق، تبيّن أن جميعها مزيفة بطريقة متقنة، إذ كانت تبدو أصلية لوجود أختام المكتبة وأرقام الجرد عليها.\n",
        "\n",
        "لم تكن مكتبة تارتو هي الضحية الوحيدة. فبعد أسابيع، اختفت أيضاً عشرة كتب نادرة من مكتبة جامعة تالين في العاصمة الإستونية.\n",
        "\n",
        "على مدار 18 شهراً، سُرقت طبعات أصلية لكتب كلاسيكية روسية وغيرها من المطبوعات الأثرية باللغة الروسية من عشرات المكتبات الأوروبية في دول البلطيق وفنلندا وسويسرا وفرنسا.\n",
        "\n",
        "في بعض الحالات، استُبدلت النسخ الأصلية بنسخ مزيفة، وفي حالات أخرى، خرجت الكتب من المكتبة ولم تعد إليها أبداً.\n",
        "\n",
        "وفي استجابة لتلك الحوادث، أطلقت وكالة الاتحاد الأوروبي للتعاون في مجال إنفاذ القانون (يوروبول) تحقيقاً يسمى بـ\"عملية بوشكين\"، نسبةً إلى سرقة أعمال الكاتب المسرحي الروسي. وشارك في التحقيق أكثر من 100 ضابط، إذ قاموا بتفتيش عدد من الممتلكات في عدة دول أوروبية. وأسفرت الحملة حتى الآن عن اعتقال تسعة من المشتبه بهم، جميعهم يحملون الجنسية الجورجية.\n",
        "\n",
        "كان بيكا تسيريكيدزه البالغ 48 عاماً، أول مشتبه به يُقبض عليه. حُوكم وأُدين بارتكاب ثلاث جرائم في بلدين مختلفين في نفس الوقت، لاتفيا وإستونيا، بينها سرقات من مكتبتي تارتو وتالين.\n",
        "\n",
        "يقضي تسيريكيدزه حالياً عقوبة السجن لمدة ثلاث سنوات وثلاثة أشهر في إستونيا، حيث يُسمح للسجناء بالتحدث إلى الصحفيين.\n",
        "\n",
        "وفي حديثه لبي بي سي قال تسيريكيدزه، إنه قرر عام 2008 الاتجار في الكتب القديمة، من خلال شرائها وترميمها وإعادة بيعها، لتوفير احتياجات أسرته.\n",
        "\n",
        "وعن التعليم الذي تلقاه للعمل كمرمم كتب، أوضح أنه \"تعلم كل شيء من خلال الممارسة\".\n",
        "\n",
        "وشبّه نفسه بأنه مثل \"ساحر\" مع الكتب. مضيفاً: \"بمجرد أن أُمسك الكتاب أستطيع أن أحدد فوراً قيمته والمبلغ الذي قد يدفعونه في المزاد من أجله\".\n",
        "\n",
        "كانت أول مواجهة لتسيريكيدزه مع القانون في عام 2016 عندما أُدين في جورجيا بتهمة سرقة كتب قديمة من متحف تبليسي التاريخي. وقد اعترف بذنبه في ذلك الوقت، ليصدر ضده حكماً بالسجن مع وقف التنفيذ.\n",
        "\n",
        "\"أكبر عملية سرقة منذ الحرب العالمية الثانية\"\n",
        "\n",
        "ي أكتوبر/تشرين الأول 2023، جلس زوجان شابان في قاعة المطالعة بمكتبة جامعة وارسو: ارتدى الرجل قبعة بيسبول سوداء وكانت المرأة ذات شعر أحمر.\n",
        "\n",
        "كان يتصفحان كتباً قديمة في المكتبة، وفي لحظة ما، كان الرجل يُقبّل رفيقته على خدها. ظهر كل هذا في تسجيل من كاميرا مراقبة.\n",
        "\n",
        "تبيّن أن الشاب هو ماتي، ابن تسيريكيدزه، أما المرأة فهي آنا غوغولادزي، زوجة ماتي.\n",
        "\n",
        "لاحقاً قبضت الشرطة عليهما بتهمة سرقة كتب من المكتبة، تُقدر قيمتها بحوالي 100 ألف دولار، وأُدينا بالجريمة.\n",
        "\n",
        "تمتلك جامعة وارسو مجموعة ثرية جداً من كتب ما قبل وما بعد الاتحاد السوفيتي. وكانت هذه المطبوعات قد نجت بأعجوبة من انتفاضة وارسو عام 1944، عندما احترق مبنى الجامعة نفسه.\n",
        "\n",
        "قال البروفيسور هيرونيم غرالا، من جامعة وارسو لبي بي سي: \"نحن الجيل الذي يدرك تماماً فكرة أن أحدهم حافظ على هذه الكتب من أجلنا ذات يوم\".\n",
        "\n",
        "خلال أقل من عام، بلغ إجمالي السرقات من مكتبة الجامعة 73 نسخة نادرة، تُقدر قيمتها بنحو 600 ألف دولار. ولم يُقبض على بعض الجناة حتى الآن.\n",
        "\n",
        "وصف البروفيسور غرالا حجم الجرائم لوسائل الإعلام، بأنها \"أكبر سرقة منذ الحرب العالمية الثانية. إنها تشبه تحطيم جوهرة تاج\".\n",
        "\n",
        "لم يلاحظ موظفو المكتبة استبدال الكتب الأصلية بالمزيفة على الفور. قال البروفيسور غرالا: \"إن الكتب المزيفة كانت تحمل الرمز نفسه (الكود)، وبنفس حجم النسخ الأصلية، ولم تكن هناك كتب مفقودة من على الرفوف\".\n",
        "\n",
        "وقال البروفيسور غرالا لبي بي سي: \"لا شك أن مجموعة اللصوص التي وصلت في البداية (كانت تستهدف كتباً محددة) وأعدت نفسها جيداً. ويبدو أنهم صنعوا نُسخاً عالية الجودة\".\n",
        "\n",
        "وأشار البروفيسور غرالا إلى أن الإصلاحات المكتبية الأخيرة التي خضعت لها هذه المكتبة ربما كانت السبب في تزايد عدد السرقات، خاصة تلك المتعلقة بتخفيف قواعد الفحص للكتب النادرة والقديمة.\n",
        "\n",
        "وتهدف هذه الإجراءات في الأصل إلى زيادة سهولة وصول القُرّاء للكتب.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_P3kW9DLfi-Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Details Extraction"
      ],
      "metadata": {
        "id": "k38wmtvugnRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ArticleCategory =Literal[\n",
        "       \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "      \"health\", \"entertainment\", \"science\",\n",
        "      \"not_specified\"\n",
        "]\n",
        "EntityType = Literal[\n",
        "        \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "      \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "  entity_value: str = Field(..., description = \"The actual name or value of the entity.\")\n",
        "  entity_type: EntityType = Field(..., description= \"The type of recognized entity.\")\n",
        "\n",
        "\n",
        "class ArticleDetails(BaseModel):\n",
        "  article_title: str = Field(..., min_length=5, max_length=100, description= \"A fully informative and SEO optimized title of the article.\")\n",
        "  article_date: datetime = Field(..., description= \"The date and time when the article was published.\")\n",
        "  article_keywords: List[str] = Field(..., min_items =1, description= \"Relevant keywords associated with the story.\")\n",
        "  article_summary: List[str] = Field(..., min_items = 1, max_items = 10, description= \"Summarized key points about the story (1-10 points).\")\n",
        "  article_category: ArticleCategory = Field(..., description= \"The category of the article.\")\n",
        "  article_entity: List[Entity] = Field(..., min_items=1, max_items=10 ,description= \"The entity associated with the article.\")\n",
        "\n",
        "\n",
        "\n",
        "# 2 role system and user\n",
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\":\"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Article:\",\n",
        "            article.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Scheme:\",\n",
        "            json.dumps(ArticleDetails.model_json_schema(), ensure_ascii=False),\n",
        "            \"\",\n",
        "\n",
        "            \"## Article Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "deEUtkfngp1b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation"
      ],
      "metadata": {
        "id": "jSe-HwBhtzwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translated_Article(BaseModel):\n",
        "  translation_title: str = Field(...,min_length = 10, max_length = 50 ,description= \"Suggested translated title of the article.\")\n",
        "  translation_content: str = Field(..., min_items = 5, description= \"Translated content of the article\")\n",
        "\n",
        "\n",
        "targted_language = \"English\"\n",
        "\n",
        "translation_messages =[\n",
        "    {\n",
        "        \"role\":\"system\",\n",
        "          \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "          ])\n",
        "    },\n",
        "    {\n",
        "      \"role\":\"user\",\n",
        "      \"content\": \"\\n\".join([\n",
        "            \"## Article:\",\n",
        "            article.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Scheme:\",\n",
        "            json.dumps(Translated_Article.model_json_schema(), ensure_ascii=False),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targted_language,\n",
        "            \"\",\n",
        "\n",
        "            \"## Article Details:\",\n",
        "            \"```json\"\n",
        "      ])\n",
        "\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "rJw2CkF5t2_M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "a9N1sEjJmEzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype= None)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "JYdAVOYhlNAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/sample_data/Qwen_Model\")\n",
        "tokenizer.save_pretrained(\"/content/sample_data/Tokenizer_Model\")"
      ],
      "metadata": {
        "id": "qUCndybt1W7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Details Extraction\n",
        "\n",
        "text = tokenizer.apply_chat_template(details_extraction_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "model_inputs = tokenizer([text],  return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024,\n",
        "                               do_sample = False,  top_k=None, temperature=None, top_p=None)\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "MLFvG_ionCQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w7vfIfcoxbq",
        "outputId": "328b886d-1802-4cfd-b5df-9943a632d141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"article_title\": \"أكبر عملية سرقة منذ الحرب العالمية الثانية\",\n",
            "  \"article_date\": \"2023-10-01T00:00:00Z\",\n",
            "  \"article_keywords\": [\n",
            "    \"سرقة\",\n",
            "    \"كتب\",\n",
            "    \"大学图书馆\",\n",
            "    \"اتحاد الأوروبي للتعاون في مجال إنفاذ القانون\"\n",
            "  ],\n",
            "  \"article_summary\": [\n",
            "    \"في أبريل/نيسان 2022، دخل شخصان مكتبة جامعة تارتو في إستونيا، وطلبا الاطلاع على ثمانية كتب لفنانين من القرن التاسع عشر.\",\n",
            "    \"تحدث الرجلان بالروسية، مؤكدين لموظف المكتبة أن الأصغر سناً منهما يُجري بحثاً لإكمال دراسته في الولايات المتحدة.\",\n",
            "    \"بعد ثلاثة أشهر، لاحظ موظفو المكتبة استبدال كتابين من الكتب الثمانية بنسخ مزيفة. وكشفت سجلات المكتبة أن الكتابين الأصليين كانا قد خرجا للمرة الأخيرة بحوزة نفس الرجلين في أبريل/نيسان.\",\n",
            "    \"سارع أمناء المكتبة إلى فحص الكتب الستة الأخرى التي تعد من الكلاسيكيات الروسية. وبعد فحص دقيق، تبيّن أن جميعها مزيفة بطريقة متقنة، إذ كانت تبدو أصلية لوجود أختام المكتبة وأرقام الجرد عليها.\",\n",
            "    \"لم تكن مكتبة تارتو هي الضحية الوحيدة. فبعد أسابيع، اختفت أيضاً عشرة كتب نادرة من مكتبة جامعة تالين في العاصمة الإستونية.\"\n",
            "  ],\n",
            "  \"article_category\": \"crime\",\n",
            "  \"article_entity\": [\n",
            "    {\n",
            "      \"entity_value\": \"ألكسندر بوشكين\",\n",
            "      \"entity_type\": \"person-male\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"نيكولاي غوغول\",\n",
            "      \"entity_type\": \"person-male\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Translation\n",
        "\n",
        "text = tokenizer.apply_chat_template(translation_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "model_inputs = tokenizer([text],  return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024,\n",
        "                               do_sample = False,  top_k=None, temperature=None, top_p=None)\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "translation_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "sOGdvmcWo9pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translation_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLjWP1W7vxyK",
        "outputId": "19e5f2d5-3b32-4a7f-d6ee-3573f87896ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"translation_title\": \"Two Estonian Students Steal Eighteenth Century Russian Literature Books\",\n",
            "  \"translation_content\": \"In April/May 2022, two students entered the library at Tallinn University in Estonia and requested to read eight books on Russian artists from the 18th century: the poet and playwright Alexander Pushkin, and the famous Russian writer with Ukrainian origins Nikolai Gogol.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Groq"
      ],
      "metadata": {
        "id": "Ub78GEIMyiIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "\n",
        "groq_client = Groq(api_key = userdata.get('groq'))\n",
        "llama_model_id = \"llama-3.1-8b-instant\""
      ],
      "metadata": {
        "id": "bG3OSIS2waaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = groq_client.chat.completions.create(messages= details_extraction_messages, model = llama_model_id, temperature=0.2)\n",
        "llama_details = chat_completion.choices[0].message.content\n",
        "print(llama_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8E1-tF6y3EO",
        "outputId": "0dfcc142-1e10-4380-8414-cf274d3b0f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"article_title\": \"سرقة كتب قديمة من مكتبات أوروبية\",\n",
            "  \"article_date\": \"2023-10-01T00:00:00\",\n",
            "  \"article_keywords\": [\n",
            "    \"سرقة كتب\",\n",
            "    \"مكتبات أوروبية\",\n",
            "    \"كتب قديمة\",\n",
            "    \"جورجيا\",\n",
            "    \"إستونيا\",\n",
            "    \"لاتفيا\",\n",
            "    \"فرنسا\",\n",
            "    \"سويسرا\",\n",
            "    \"فنلندا\"\n",
            "  ],\n",
            "  \"article_summary\": [\n",
            "    \"تم سرقة كتب قديمة من مكتبات أوروبية\",\n",
            "    \"تم اكتشاف سرقة كتب من مكتبة جامعة تارتو في إستونيا\",\n",
            "    \"تم اكتشاف سرقة كتب من مكتبة جامعة تالين في العاصمة الإستونية\",\n",
            "    \"تم اكتشاف سرقة كتب من مكتبة جامعة وارسو في بولندا\",\n",
            "    \"تم اكتشاف سرقة كتب من مكتبات أخرى في أوروبا\",\n",
            "    \"تم اعتقال تسعة أشخاص بتهمة سرقة الكتب\",\n",
            "    \"تم اكتشاف أن السارقين كانوا من جورجيا\",\n",
            "    \"تم اكتشاف أن السارقين كانوا يعملون معًا\",\n",
            "    \"تم اكتشاف أن السارقين كانوا يستخدمون تقنيات متقدمة لسرقة الكتب\",\n",
            "    \"تم اكتشاف أن السارقين كانوا يبيعون الكتب المسرقة على الإنترنت\"\n",
            "  ],\n",
            "  \"article_category\": \"ثقافة\",\n",
            "  \"article_entity\": [\n",
            "    {\n",
            "      \"entity_value\": \"بيكا تسيريكيدزه\",\n",
            "      \"entity_type\": \"person-male\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"ماتي تسيريكيدزه\",\n",
            "      \"entity_type\": \"person-male\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"آنا غوغولادزي\",\n",
            "      \"entity_type\": \"person-female\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"جامعة تارتو\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"جامعة تالين\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"جامعة وارسو\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"إستونيا\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"لاتفيا\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"فرنسا\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"سويسرا\",\n",
            "      \"entity_type\": \"location\"\n",
            "    },\n",
            "    {\n",
            "      \"entity_value\": \"فنلندا\",\n",
            "      \"entity_type\": \"location\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = groq_client.chat.completions.create(messages= translation_messages, model = llama_model_id, temperature=0.2)\n",
        "llama_translation = chat_completion.choices[0].message.content\n",
        "print(llama_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzRBXFo-6I7z",
        "outputId": "657cbaca-0a40-4836-d95d-0b165e30ec9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"translation_title\": \"Estonian University Libraries Hit by Rare Book Heists\",\n",
            "  \"translation_content\": \"In April 2022, two men entered the University of Tartu library in Estonia, requesting to view eight books by 19th-century Russian artists, including poet and playwright Alexander Pushkin and famous Russian writer of Ukrainian origin, Nikolai Gogol. The men spoke in Russian, assuring the librarian that the younger of the two was conducting research to complete his studies in the United States. Three months later, library staff noticed that two of the eight books had been replaced with fake copies. The library's records showed that the original books had last been borrowed by the same two men in April. The library administrators quickly inspected the six other books, which were all classics of Russian literature. After a thorough examination, it was found that all of them were fake, with a high level of sophistication, as they looked original due to the presence of library stamps and inventory numbers. Tartu was not the only victim. Weeks later, ten rare books also disappeared from the University of Tallinn library in the Estonian capital. Over the course of 18 months, original editions of classic Russian literature and other rare publications in Russian were stolen from dozens of European libraries in the Baltic states, Finland, Switzerland, and France. In some cases, the original copies were replaced with fake ones, while in other cases, the books were taken from the library and never returned. In response to these incidents, Europol launched an investigation called 'Operation Pushkin,' named after the theft of the Russian playwright's works. More than 100 officers participated in the investigation, conducting searches at several properties in multiple European countries. The operation has so far resulted in the arrest of nine suspects, all of whom are Georgian nationals. The first suspect to be arrested was Beka Tsiskaridze, 48, who was convicted of committing three crimes in two different countries, Latvia and Estonia, including thefts from the libraries of Tartu and Tallinn. Tsiskaridze is currently serving a three-year and three-month prison sentence in Estonia, where prisoners are allowed to speak to the media. In an interview with the BBC, Tsiskaridze said he decided to deal in old books in 2008, buying, restoring, and reselling them to support his family. He claimed to have learned everything he needed to know about restoring books through practice. Tsiskaridze compared himself to a magician with books, saying, 'As soon as I touch a book, I can immediately determine its value and the amount they might pay for it at an auction.' Tsiskaridze's first run-in with the law was in 2016, when he was convicted in Georgia of stealing old books from the Tbilisi History Museum. He admitted to the crime at the time and was given a suspended sentence. 'The biggest heist since World War II' In October 2023, a couple in their twenties sat in a reading room at the University of Warsaw library: the man wore a black baseball cap, and the woman had red hair. They were browsing through old books in the library, and at one point, the man kissed his girlfriend on the cheek. This was all captured on a security camera. It turned out that the young man was Mateusz, Tsiskaridze's son, and the woman was Anna Gogoladze, Mateusz's wife. The police later arrested them on charges of stealing books from the library, worth around $100,000. The University of Warsaw has a vast collection of pre- and post-Soviet literature. The books had miraculously survived the 1944 Warsaw Uprising, when the university building burned down. Professor Hieronim Grala, from the University of Warsaw, told the BBC: 'We are the generation that fully understands the idea that someone has preserved these books for us one day.' Within a year, a total of 73 rare copies were stolen from the library, worth around $600,000. Some of the perpetrators have not yet been caught. Professor Grala described the scale of the crimes to the media as 'the biggest heist since World War II. It's like destroying a crown jewel.' Library staff did not immediately notice that the original books had been replaced with fake ones. Professor Grala said: 'The fake books had the same code, the same size as the originals, and there were no missing books on the shelves.' Professor Grala told the BBC: 'There is no doubt that the group that first arrived (targeted specific books) and then reorganized themselves. It seems they made high-quality copies.' Professor Grala pointed out that the recent library renovations, which included relaxing the rules for inspecting rare and old books, may have contributed to the increase in thefts, especially those related to books with specific codes. These measures were intended to make it easier for readers to access the books.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distilation"
      ],
      "metadata": {
        "id": "1Vy0FHI_B-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_path = join(data_dir, \"datasets\", \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "\n",
        "for line in open(raw_data_path):\n",
        "  if line.strip() ==\"\":\n",
        "    continue\n",
        "  raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "random.Random(44).shuffle(raw_data)\n",
        "print(f\"Raw data: {len(raw_data)} \\n {raw_data[0]['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ82Wg2j7Wec",
        "outputId": "2d552ca1-11ce-48a8-906f-2d8ee032c1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: 2400 \n",
            " قالت هيومن منظمة رايتس ووتش إن جماعة إم 23 المسلحة المدعومة من رواندا نفذت إعدامات بإجراءات موجزة وتجنيدا قسريا للمدنيين في شرق جمهورية الكونغو الديمقراطية. \n",
            " ووفق المنظمة، فإن الجيش الكونغولي يرد على هجوم حركة إم 23 من خلال التعاون مع المليشيات العرقية ذات السجلات المسيئة. \n",
            " وقال توماس فيسي، الباحث الأول في المنظمة، إن متمردي حركة إم 23 المدعومة من رواندا في شمال كيفو يتركون وراءهم سلسلة متزايدة من جرائم الحرب ضد المدنيين. \n",
            " وطالب رواندا بإنهاء دعمها العسكري للحركة، والقوات الحكومية الكونغولية بإعطاء الأولوية لحماية المدنيين والتوقف عن استخدام المليشيات المسيئة كقوات بالوكالة. \n",
            " وتوفر التحقيقات الأخيرة التي أجراها فريق خبراء الأمم المتحدة المعني بالكونغو، وكذلك أبحاث هيومن رايتس ووتش، صورا فوتوغرافية مهمة وغيرها من الأدلة على أن رواندا لا تقدم الدعم اللوجستي لحركة إم 23 فحسب، بل إن القوات الرواندية تدعم أو تقاتل إلى جانب الجماعة المسلحة داخل الكونغو الديمقراطية. ونفت الحكومة الرواندية دعم متمردي حركة إم 23. \n",
            " أجبرت الأعمال العدائية المتجددة من قبل حركة إم 23 والجيش الكونغولي ومختلف الجماعات المسلحة الأخرى أكثر من 520 ألف شخص على الفرار من ديارهم، وفقا للأمم المتحدة. \n",
            " وقد أدى ذلك إلى تفاقم الوضع الأمني والإنساني الكارثي بالفعل في شمال كيفو والمنطقة الشرقية الأوسع. \n",
            " وحذرت منظمة أطباء بلا حدود الإنسانية من كارثة صحية محتملة مع الانتشار السريع للكوليرا في مخيمات النازحين خارج غوما، عاصمة مقاطعة شمال كيفو.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"llama-3.1-8b-instant\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "ix = 0\n",
        "\n",
        "output_save = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "\n",
        "\n",
        "for article in tqdm(raw_data):\n",
        "  details_extraction = [\n",
        "      {\n",
        "        \"role\":\"system\",\n",
        "        \"content\":\"/n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\":\"user\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"## Article:\",\n",
        "            article[\"content\"].strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Scheme:\",\n",
        "            json.dumps(ArticleDetails.model_json_schema(), ensure_ascii=False),\n",
        "            \"\",\n",
        "\n",
        "            \"## Article Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "  ]\n",
        "  try:\n",
        "    extract_response = groq_client.chat.completions.create(\n",
        "            messages=details_extraction,\n",
        "            model=model_id,\n",
        "            temperature=0.1\n",
        "        )\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    break\n",
        "\n",
        "  if extract_response.choices[0].finish_reason != \"stop\":\n",
        "    prompt_tokens += extract_response.usage.prompt_tokens\n",
        "    continue\n",
        "\n",
        "  llm_response = extract_response.choices[0].message.content\n",
        "  llm_response_dict = repaire_json(llm_response)\n",
        "\n",
        "  if not llm_response_dict:\n",
        "    continue\n",
        "\n",
        "  with open(output_save, \"a\", encoding=\"utf-8\") as sft:\n",
        "    sft.write(json.dumps({\n",
        "        \"id\":ix,\n",
        "        \"article\":article[\"content\"].strip(),\n",
        "        \"task\": \"Extrat the article details into a JSON.\",\n",
        "        \"output_schema\": json.dumps(ArticleDetails.model_json_schema(), ensure_ascii=False),\n",
        "        \"response\":llm_response_dict\n",
        "    }, ensure_ascii = False, default = str) + \"\\n\")\n",
        "\n",
        "  ix+=1\n",
        "  prompt_tokens += extract_response.usage.prompt_tokens\n",
        "  completion_tokens += extract_response.usage.completion_tokens\n",
        "\n",
        "  if(x % 3) == 0 :\n",
        "    cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "    cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "    total_cost = cost_input + cost_output\n",
        "    print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")\n"
      ],
      "metadata": {
        "id": "4RXeH6PeFMK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"llama-3.1-8b-instant\"\n",
        "price_per_1m_input_tokens = 0.150\n",
        "price_per_1m_output_tokens = 0.600\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "ix = 0\n",
        "\n",
        "output_save = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "\n",
        "for article in tqdm(raw_data):\n",
        "    for targeted_lang in [\"English\", \"French\"]:\n",
        "        translation_role = [\n",
        "            {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"\\n\".join([\n",
        "                  \"You are a professional translator.\",\n",
        "                  \"You will be provided by an Arabic text.\",\n",
        "                  \"You have to translate the text into the `Targeted Language`.\",\n",
        "                  \"Follow the provided Scheme to generate a JSON\",\n",
        "                  \"Do not generate any introduction or conclusion.\"\n",
        "                ])\n",
        "          },\n",
        "          {\n",
        "              \"role\":\"user\",\n",
        "              \"content\":\"\\n\".join([\n",
        "                  \"## Article:\",\n",
        "                  article[\"content\"].strip(),\n",
        "                  \"\",\n",
        "\n",
        "                  \"## Pydantic Scheme:\",\n",
        "                  json.dumps(Translated_Article.model_json_schema(), ensure_ascii=False),\n",
        "                  \"\",\n",
        "\n",
        "                  \"## Targeted Language:\",\n",
        "                  targeted_lang,\n",
        "                  \"\",\n",
        "\n",
        "                  \"## Article Details:\",\n",
        "                  \"```json\"\n",
        "              ])\n",
        "          }\n",
        "        ]\n",
        "        try:\n",
        "          translate_response = groq_client.chat.completions.create(\n",
        "                  messages=translation_role,\n",
        "                  model=model_id,\n",
        "                  temperature=0.1\n",
        "              )\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          break\n",
        "\n",
        "        if translate_response.choices[0].finish_reason != \"stop\":\n",
        "          prompt_tokens += translate_response.usage.prompt_tokens\n",
        "          continue\n",
        "\n",
        "        llm_translate = translate_response.choices[0].message.content\n",
        "        llm_translate_dict = repaire_json(llm_translate)\n",
        "\n",
        "        if not llm_translate_dict:\n",
        "          continue\n",
        "\n",
        "        with open(output_save, \"a\", encoding=\"utf-8\") as sft:\n",
        "          sft.write(json.dumps({\n",
        "              \"id\":ix,\n",
        "              \"article\":article[\"content\"].strip(),\n",
        "              \"task\": f\"You have to translate the story content into {targeted_lang} associated with a title into a JSON.\",\n",
        "              \"output_schema\": json.dumps(Translated_Article.model_json_schema(), ensure_ascii=False),\n",
        "              \"response\":llm_translate_dict\n",
        "          }, ensure_ascii = False, default = str) + \"\\n\")\n",
        "\n",
        "        ix+=1\n",
        "        prompt_tokens += translate_response.usage.prompt_tokens\n",
        "        completion_tokens += translate_response.usage.completion_tokens\n",
        "\n",
        "        if(x % 3) == 0 :\n",
        "          cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "          cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "          total_cost = cost_input + cost_output\n",
        "          print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")\n"
      ],
      "metadata": {
        "id": "u7QPzJcJLDn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure Output on Dataset\n"
      ],
      "metadata": {
        "id": "DgV191c3WDAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_data_path = join(data_dir, \"datasets\", \"sft.jsonl\")\n",
        "llm_ft_data = []\n",
        "\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "  if line.strip() == \"\":\n",
        "    continue\n",
        "  rec = json.loads(line.strip())\n",
        "\n",
        "  llm_ft_data.append({\n",
        "      \"system\": system_message,\n",
        "      \"instruction\": \"\\n\".join({\n",
        "            \"# Article:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "      }),\n",
        "      \"input\": \"\",\n",
        "      \"output\":\"\\n\".join({\n",
        "          \"```json\",\n",
        "          json.dumps(rec[\"response\"], ensure_ascii = False, default = str),\n",
        "          \"```\"\n",
        "      }),\n",
        "      \"history\":[]\n",
        "        })\n",
        "\n",
        "random.Random(44).shuffle(llm_ft_data)"
      ],
      "metadata": {
        "id": "4-jWvWRKUR3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split & Train data"
      ],
      "metadata": {
        "id": "kq3EieAjvZAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(llm_ft_data)\n",
        "\n",
        "# Splitting data\n",
        "train_sample_size = 2700\n",
        "train_ds = llm_ft_data[:train_sample_size]\n",
        "eval_ds = llm_ft_data[train_sample_size:]\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "  json.dump(train_ds, dest, ensure_ascii = False, default = str)\n",
        "\n",
        "with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding = \"utf8\") as dest:\n",
        "  json.dump(eval_ds, dest, ensure_ascii = False, default = str)"
      ],
      "metadata": {
        "id": "SwWkacJwt4CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "im-6unrc28Nf",
        "outputId": "75d8f334-3bf5-4fb0-bcc3-a2499c087576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/FineTuning_fromScratch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning\n"
      ],
      "metadata": {
        "id": "FXK2fjt0z6P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/articl_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: articls_finetune_train\n",
        "eval_dataset: articls_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "output_dir: /gdrive/MyDrive/FineTuning_fromScratch/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: news-finetune-llamafactory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU3bhIm2z8tc",
        "outputId": "da562d66-c113-41e2-ecf6-95351f2d3abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/articl_finetune.yaml"
      ],
      "metadata": {
        "id": "Y9zHngrw3Y7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = None\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "2g_Gx6Tp3hFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/FineTuning_fromScratch/models\"\n",
        "finetuned_model_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M3EFkkeVLiuH",
        "outputId": "b1dc0c0d-b571-44d8-ff35-b5dfbde3ee37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/FineTuning_fromScratch/models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/FineTuning_fromScratch/models\"\n",
        "model.load_adapter(finetuned_model_id)"
      ],
      "metadata": {
        "id": "CGiBdY22Hsd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Details Extraction & Translation after Fine-Tuning\n",
        "\n",
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(details_extraction_messages)"
      ],
      "metadata": {
        "id": "kvadEgWGHNKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repaire_json(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG9ozcjuMKN-",
        "outputId": "6619b26a-c500-4d3f-c579-aaef7bf34073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article_title': 'سرقة كتب قديمة من مكتبات جامعة تارتو ووالين في إستونيا',\n",
              " 'article_date': '2023-10-01',\n",
              " 'article_keywords': ['سرقة',\n",
              "  'كتب قديمة',\n",
              "  'مكتبات',\n",
              "  'جامعة تارتو',\n",
              "  'جامعة وارسو'],\n",
              " 'article_summary': ['دخل شخصان مكتبة جامعة تارتو في إستونيا وطلبا الاطلاع على ثمانية كتب.',\n",
              "  'استبدال كتابين من الكتب الثمانية بنسخ مزيفة بعد ثلاثة أشهر.',\n",
              "  'اختفت أيضاً عشرة كتب نادرة من مكتبة جامعة تالين في العاصمة الإستونية.',\n",
              "  'تم إطلاق وكالة الاتحاد الأوروبي للتعاون في مجال إنفاذ القانون تحقيقاً يسمى بـ\"عملية بوشكين\".',\n",
              "  'تم اعتقال تسعة من المشتبه بهم، جميعهم يحملون الجنسية الجورجية.'],\n",
              " 'article_category': 'art',\n",
              " 'article_entity': [{'entity_value': 'جامعة تارتو',\n",
              "   'entity_type': 'organization'},\n",
              "  {'entity_value': 'ألكسندر بوشكين', 'entity_type': 'person-male'},\n",
              "  {'entity_value': 'نيكولاي غوغول', 'entity_type': 'person-male'},\n",
              "  {'entity_value': 'جامعة وارسو', 'entity_type': 'organization'},\n",
              "  {'entity_value': 'ماتي', 'entity_type': 'person-male'},\n",
              "  {'entity_value': 'آنا غوغولادزي', 'entity_type': 'person-female'},\n",
              "  {'entity_value': 'يوروبول', 'entity_type': 'organization'},\n",
              "  {'entity_value': 'البروفيسور هيرونيم غرالا', 'entity_type': 'person-male'},\n",
              "  {'entity_value': 'الحرب العالمية الثانية', 'entity_type': 'event'},\n",
              "  {'entity_value': 'إستونيا', 'entity_type': 'location'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM"
      ],
      "metadata": {
        "id": "175T6lqa6EIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this on termial\n",
        "#vllm serve \"Qwen/Qwen2.5-1.5B-Instruct\" --dtype=half --gpu-memory-utilization 0.8 --max_lora_rank 64 --enable-lora --lora-modules articls-lora=\"/gdrive/MyDrive/FineTuning_fromScratch/models\""
      ],
      "metadata": {
        "id": "OeeE2ymQ6FqK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(translation_messages, tokenize=False, add_generation_prompt=True)"
      ],
      "metadata": {
        "id": "hRVG32amEwmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_model_id = \"articls-lora\"\n",
        "\n",
        "llm_response = requests.post(\"http://localhost:8000/v1/completions\", json={\n",
        "    \"model\": vllm_model_id,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3\n",
        "})\n",
        "\n",
        "llm_response.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqFp5C20E9Z7",
        "outputId": "61591054-93c1-44e3-f952-aa6047bbde44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-0a7aec40e6e14175a4e7ab15059f1320',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1756914258,\n",
              " 'model': 'articls-lora',\n",
              " 'choices': [{'index': 0,\n",
              "   'text': '```json{\"translated_title\": \"The Case of Book Thieves: A Heinous Crime in Europe\", \"translated_content\": \"In April 2022, two individuals entered the University of Tartu library in Estonia and requested to view eight books by Russian 19th-century artists: the Russian playwright and theater writer Alexander Pushkin, and the famous Russian-born Ukrainian novelist Nikolai Gogol.\\\\n\\\\nThe two men spoke Russian, confirming to the library staff that the younger of the two was conducting research to complete his studies in the United States. Three months later, library staff noticed the replacement of two of the eight books with copies. The library records revealed that the original copies had last been in the possession of the same men in April.\\\\n\\\\nThe library\\'s custodians quickly checked the six other books that are considered classics of Russian literature. After a thorough examination, it was determined that all of them were counterfeit in a meticulous manner, as they appeared genuine due to the presence of library seals and serial numbers.\\\\n\\\\nTartu University was not the only victim; ten rare books from the University of Tallinn in the Estonian capital also vanished. Over 18 months, original prints of classic Russian books and other archaeological publications in Russian were stolen from dozens of European libraries in the Baltic states, Finland, Switzerland, and France.\\\\n\\\\nIn some cases, the original copies were replaced with counterfeit versions, and in others, the books were withdrawn from the library and never returned.\\\\n\\\\nIn response to these incidents, the European Union Agency for Law Enforcement Cooperation (Europol) launched an investigation called the Pushkin Case, in reference to the theft of works by the Russian playwright. Over 100 officers participated in the investigation, conducting searches of several possessions in several European countries. The campaign so far has resulted in the arrest of nine suspects, all of whom hold Georgian citizenship.\\\\n\\\\nBica Tserikidzhah, 48, was the first suspect arrested. He was convicted and sentenced to three years and three months in prison in Estonia, where prisoners are allowed to speak to journalists.\\\\n\\\\nIn an interview with BBC, Tserikidzhah stated that he decided in 2008 to trade in old books by buying, restoring, and reselling them to meet the needs of his family.\\\\n\\\\nRegarding the education he received to work as a book restorer, he explained that he learned everything through practice.\\\\n\\\\nHe likened himself to a magician with books. Adding, \\\\\"As soon as I hold a book, I can immediately determine its value and the amount that may be paid at auction.\\\\\"\\\\n\\\\nTserikidzhah\\'s first encounter with the law occurred in 2016 when he was convicted in Georgia of stealing old books from the historic Tbilisi Museum. He admitted his guilt at that time, receiving a suspended sentence.\\\\n\\\\n\\\\\"The largest book theft since World War II\\\\\"\\\\n\\\\nOn October 2023, a pair of young men sat in the reading room of the University of Warsaw library: the man wore a dark baseball cap, and the woman had red hair.\\\\n\\\\nThey were browsing old books in the library, and at one point, the man kissed his companion on the cheek. All this was captured on video from a surveillance camera.\\\\n\\\\nTserikidzhah was identified as Mati, the son of the suspect, while the woman was Anna Gogoladze, his wife.\\\\n\\\\nLater, police arrested them on charges of stealing books from the library, estimated to be worth about $100,000, and they were convicted of the crime.\\\\n\\\\nThe University of Warsaw holds a very rich collection of pre- and post-Soviet books. These publications had miraculously survived the 1944 Warsaw Uprising, when the university building itself burned down.\\\\n\\\\nProfessor Hironim Ghalia from the University of Warsaw told BBC, \\\\\"We are the generation who truly understand the idea that one person preserved these books for us one day.\\\\\"\\\\n\\\\nWithin less than a year, the total theft of seven rare copies of books estimated to be worth about $600,000 had occurred. Some criminals have not yet been caught.\\\\n\\\\nProfessor Ghalia described the size of the crimes to the media as \\\\\"the largest theft since World War II. It is like breaking a jewel from a crown.\\\\\"\\\\n\\\\nLibrary staff did not notice the replacement of the original books with counterfeit copies immediately. Professor Ghalia stated, \\\\\"The counterfeit books had the same code (the key) and the same size as the original copies, and there were no missing copies on the shelves.\\\\\"\\\\n\\\\nProfessor Ghalia told BBC, \\\\\"There is no doubt that the gang that initially targeted specific books has repurposed itself',\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length',\n",
              "   'stop_reason': None,\n",
              "   'prompt_logprobs': None}],\n",
              " 'usage': {'prompt_tokens': 1770,\n",
              "  'total_tokens': 2770,\n",
              "  'completion_tokens': 1000,\n",
              "  'prompt_tokens_details': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load HTML Test LLM"
      ],
      "metadata": {
        "id": "Ln4CLs8QS560"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile locust.py\n",
        "\n",
        "import random\n",
        "import json\n",
        "from locust import HttpUser, task, between, constant\n",
        "from transformers import AutoTokenizer\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker('ar')\n",
        "\n",
        "class Load_Test(HttpUser):\n",
        "    wait_time = between(1, 3)\n",
        "\n",
        "    @task\n",
        "    def post_test_llm(self):\n",
        "        model_id = \"articls-lora\"\n",
        "        prompt = fake.text(max_nb_chars=random.randint(150, 200))\n",
        "\n",
        "        message = {\n",
        "            \"model\": model_id,\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": 512,\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "\n",
        "        llm_response = self.client.post(\"/v1/completions\", json=message)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JekuL5dJQSp6",
        "outputId": "d1add7a6-12cc-47f1-b62c-94102859e6f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing locust.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!locust --headless -f locust.py --host=http://localhost:8000 -u 20 -r 1 -t \"60s\" --html=Test_Speed_LLM.html"
      ],
      "metadata": {
        "id": "DW0Z-czkQVZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HhbrbU30SEcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}